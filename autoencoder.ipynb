{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from numpy.random import seed\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import Model\n",
    "from tensorflow.python.client import device_lib\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "X = np.random.randn(4,size)\n",
    "X2 = np.random.randn(4,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = [1]*int(size/3) + [2]*int(size/3) + [3]*int(size/3) + [1.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro definimos as variáveis utilizadas:\n",
    "\n",
    "X0, X1, X2, X3 variáveis aleatórias geradas por uma distribuição normal\n",
    "\n",
    "X4 Variável em que 1/3 os valores são -1, 1/3 são 0 e 1/3 são 1\n",
    "\n",
    "\n",
    "Depois definimos a função para o Y dada por:\n",
    "\n",
    "Y = X0 * X1 + X2 * X3 + X1 * X3 + X4 * X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X[0]*X[1]+X[2]*X[3]+X[1]*X[3] + X4*((X[1])-X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y2 = X2[0]+X2[1]+X2[2]+X2[3]+X2[0]*X2[1] + X4*((X2[1])-X2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.transpose(X),columns= ['X0','X1','X2', 'X3'])\n",
    "X2 = pd.DataFrame(np.transpose(X2),columns= ['X0','X1','X2', 'X3'])\n",
    "X['X4'] = X4\n",
    "X2['X4'] = X4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos os dados em treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/pandas/core/frame.py:4170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "x4_train = X_train['X4']\n",
    "X_train.drop('X4',axis=1,inplace=True)\n",
    "x4_test = X_test['X4']\n",
    "X_test.drop('X4',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "     X2, Y2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = pd.DataFrame(np.transpose([y_train, y2_train]), columns = ['y1','y2'])\n",
    "ys_test =  pd.DataFrame(np.transpose([y_test, y2_test]), columns = ['y1','y2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x42_train = X2_train['X4']\n",
    "X2_train.drop('X4',axis=1,inplace=True)\n",
    "x42_test = X2_test['X4']\n",
    "X2_test.drop('X4',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamos um modelo de rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(50,50,50,50,50),activation='relu',max_iter=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.21918171\n",
      "Iteration 2, loss = 0.72170945\n",
      "Iteration 3, loss = 0.70660306\n",
      "Iteration 4, loss = 0.69664204\n",
      "Iteration 5, loss = 0.69700451\n",
      "Iteration 6, loss = 0.69388122\n",
      "Iteration 7, loss = 0.69142644\n",
      "Iteration 8, loss = 0.68944605\n",
      "Iteration 9, loss = 0.68854729\n",
      "Iteration 10, loss = 0.69035558\n",
      "Iteration 11, loss = 0.68636595\n",
      "Iteration 12, loss = 0.68928829\n",
      "Iteration 13, loss = 0.68730459\n",
      "Iteration 14, loss = 0.68960087\n",
      "Iteration 15, loss = 0.68338110\n",
      "Iteration 16, loss = 0.68602689\n",
      "Iteration 17, loss = 0.68503903\n",
      "Iteration 18, loss = 0.68352283\n",
      "Iteration 19, loss = 0.68290488\n",
      "Iteration 20, loss = 0.68515153\n",
      "Iteration 21, loss = 0.68411967\n",
      "Iteration 22, loss = 0.68141455\n",
      "Iteration 23, loss = 0.68037396\n",
      "Iteration 24, loss = 0.67725685\n",
      "Iteration 25, loss = 0.67830519\n",
      "Iteration 26, loss = 0.67773855\n",
      "Iteration 27, loss = 0.67693444\n",
      "Iteration 28, loss = 0.68092304\n",
      "Iteration 29, loss = 0.67844774\n",
      "Iteration 30, loss = 0.67823758\n",
      "Iteration 31, loss = 0.67690724\n",
      "Iteration 32, loss = 0.67496383\n",
      "Iteration 33, loss = 0.67870080\n",
      "Iteration 34, loss = 0.67381180\n",
      "Iteration 35, loss = 0.67397698\n",
      "Iteration 36, loss = 0.67432968\n",
      "Iteration 37, loss = 0.67558189\n",
      "Iteration 38, loss = 0.67660864\n",
      "Iteration 39, loss = 0.67498882\n",
      "Iteration 40, loss = 0.67318065\n",
      "Iteration 41, loss = 0.67500777\n",
      "Iteration 42, loss = 0.67429322\n",
      "Iteration 43, loss = 0.67522591\n",
      "Iteration 44, loss = 0.67155100\n",
      "Iteration 45, loss = 0.67621124\n",
      "Iteration 46, loss = 0.67302510\n",
      "Iteration 47, loss = 0.67216800\n",
      "Iteration 48, loss = 0.67154164\n",
      "Iteration 49, loss = 0.67403877\n",
      "Iteration 50, loss = 0.67375546\n",
      "Iteration 51, loss = 0.67100488\n",
      "Iteration 52, loss = 0.67142162\n",
      "Iteration 53, loss = 0.67120872\n",
      "Iteration 54, loss = 0.66909628\n",
      "Iteration 55, loss = 0.67122316\n",
      "Iteration 56, loss = 0.67159156\n",
      "Iteration 57, loss = 0.66807182\n",
      "Iteration 58, loss = 0.67228746\n",
      "Iteration 59, loss = 0.66873403\n",
      "Iteration 60, loss = 0.66864963\n",
      "Iteration 61, loss = 0.66883261\n",
      "Iteration 62, loss = 0.66759157\n",
      "Iteration 63, loss = 0.66895826\n",
      "Iteration 64, loss = 0.66852317\n",
      "Iteration 65, loss = 0.66873075\n",
      "Iteration 66, loss = 0.66869292\n",
      "Iteration 67, loss = 0.66775411\n",
      "Iteration 68, loss = 0.66774093\n",
      "Iteration 69, loss = 0.66815893\n",
      "Iteration 70, loss = 0.66536584\n",
      "Iteration 71, loss = 0.66619936\n",
      "Iteration 72, loss = 0.66648857\n",
      "Iteration 73, loss = 0.66761544\n",
      "Iteration 74, loss = 0.66844612\n",
      "Iteration 75, loss = 0.66495110\n",
      "Iteration 76, loss = 0.66431853\n",
      "Iteration 77, loss = 0.66400965\n",
      "Iteration 78, loss = 0.66796680\n",
      "Iteration 79, loss = 0.66710935\n",
      "Iteration 80, loss = 0.66173872\n",
      "Iteration 81, loss = 0.66490700\n",
      "Iteration 82, loss = 0.66433677\n",
      "Iteration 83, loss = 0.66332398\n",
      "Iteration 84, loss = 0.66184103\n",
      "Iteration 85, loss = 0.66344901\n",
      "Iteration 86, loss = 0.66474891\n",
      "Iteration 87, loss = 0.66284017\n",
      "Iteration 88, loss = 0.66123817\n",
      "Iteration 89, loss = 0.66238425\n",
      "Iteration 90, loss = 0.66056848\n",
      "Iteration 91, loss = 0.66321295\n",
      "Iteration 92, loss = 0.66083783\n",
      "Iteration 93, loss = 0.66308712\n",
      "Iteration 94, loss = 0.66227281\n",
      "Iteration 95, loss = 0.66115124\n",
      "Iteration 96, loss = 0.66282194\n",
      "Iteration 97, loss = 0.66050263\n",
      "Iteration 98, loss = 0.66157624\n",
      "Iteration 99, loss = 0.66051288\n",
      "Iteration 100, loss = 0.65926784\n",
      "Iteration 101, loss = 0.65863167\n",
      "Iteration 102, loss = 0.66141602\n",
      "Iteration 103, loss = 0.66140794\n",
      "Iteration 104, loss = 0.65830948\n",
      "Iteration 105, loss = 0.65845398\n",
      "Iteration 106, loss = 0.66057130\n",
      "Iteration 107, loss = 0.65754648\n",
      "Iteration 108, loss = 0.65936756\n",
      "Iteration 109, loss = 0.65720449\n",
      "Iteration 110, loss = 0.65659194\n",
      "Iteration 111, loss = 0.65746074\n",
      "Iteration 112, loss = 0.65832968\n",
      "Iteration 113, loss = 0.65710703\n",
      "Iteration 114, loss = 0.65675476\n",
      "Iteration 115, loss = 0.65656979\n",
      "Iteration 116, loss = 0.65843389\n",
      "Iteration 117, loss = 0.65563495\n",
      "Iteration 118, loss = 0.65623674\n",
      "Iteration 119, loss = 0.65551843\n",
      "Iteration 120, loss = 0.65274544\n",
      "Iteration 121, loss = 0.65538210\n",
      "Iteration 122, loss = 0.65408219\n",
      "Iteration 123, loss = 0.65395513\n",
      "Iteration 124, loss = 0.65314650\n",
      "Iteration 125, loss = 0.65357850\n",
      "Iteration 126, loss = 0.65204818\n",
      "Iteration 127, loss = 0.65468508\n",
      "Iteration 128, loss = 0.65456797\n",
      "Iteration 129, loss = 0.65424528\n",
      "Iteration 130, loss = 0.65250110\n",
      "Iteration 131, loss = 0.65161778\n",
      "Iteration 132, loss = 0.65334360\n",
      "Iteration 133, loss = 0.65207433\n",
      "Iteration 134, loss = 0.65479623\n",
      "Iteration 135, loss = 0.65173241\n",
      "Iteration 136, loss = 0.65137394\n",
      "Iteration 137, loss = 0.65086233\n",
      "Iteration 138, loss = 0.65071115\n",
      "Iteration 139, loss = 0.65101976\n",
      "Iteration 140, loss = 0.64918699\n",
      "Iteration 141, loss = 0.64949666\n",
      "Iteration 142, loss = 0.65187021\n",
      "Iteration 143, loss = 0.64929895\n",
      "Iteration 144, loss = 0.64983943\n",
      "Iteration 145, loss = 0.64936467\n",
      "Iteration 146, loss = 0.64947802\n",
      "Iteration 147, loss = 0.64825586\n",
      "Iteration 148, loss = 0.64842381\n",
      "Iteration 149, loss = 0.64680685\n",
      "Iteration 150, loss = 0.65120312\n",
      "Iteration 151, loss = 0.64741822\n",
      "Iteration 152, loss = 0.64700693\n",
      "Iteration 153, loss = 0.64687181\n",
      "Iteration 154, loss = 0.64743213\n",
      "Iteration 155, loss = 0.64722220\n",
      "Iteration 156, loss = 0.64549432\n",
      "Iteration 157, loss = 0.64663892\n",
      "Iteration 158, loss = 0.64629989\n",
      "Iteration 159, loss = 0.64452814\n",
      "Iteration 160, loss = 0.64413093\n",
      "Iteration 161, loss = 0.64453199\n",
      "Iteration 162, loss = 0.64496976\n",
      "Iteration 163, loss = 0.64323556\n",
      "Iteration 164, loss = 0.64525139\n",
      "Iteration 165, loss = 0.64517757\n",
      "Iteration 166, loss = 0.64410955\n",
      "Iteration 167, loss = 0.64418005\n",
      "Iteration 168, loss = 0.64271899\n",
      "Iteration 169, loss = 0.64052611\n",
      "Iteration 170, loss = 0.64196004\n",
      "Iteration 171, loss = 0.64079568\n",
      "Iteration 172, loss = 0.64191870\n",
      "Iteration 173, loss = 0.64157869\n",
      "Iteration 174, loss = 0.64305048\n",
      "Iteration 175, loss = 0.64169373\n",
      "Iteration 176, loss = 0.64003303\n",
      "Iteration 177, loss = 0.64075118\n",
      "Iteration 178, loss = 0.64024019\n",
      "Iteration 179, loss = 0.63969650\n",
      "Iteration 180, loss = 0.63848329\n",
      "Iteration 181, loss = 0.64113154\n",
      "Iteration 182, loss = 0.63938970\n",
      "Iteration 183, loss = 0.63834300\n",
      "Iteration 184, loss = 0.63841730\n",
      "Iteration 185, loss = 0.63718438\n",
      "Iteration 186, loss = 0.63680501\n",
      "Iteration 187, loss = 0.63603147\n",
      "Iteration 188, loss = 0.63821271\n",
      "Iteration 189, loss = 0.63801206\n",
      "Iteration 190, loss = 0.63636200\n",
      "Iteration 191, loss = 0.63523763\n",
      "Iteration 192, loss = 0.63857435\n",
      "Iteration 193, loss = 0.63905096\n",
      "Iteration 194, loss = 0.63679721\n",
      "Iteration 195, loss = 0.63834186\n",
      "Iteration 196, loss = 0.63423157\n",
      "Iteration 197, loss = 0.63435188\n",
      "Iteration 198, loss = 0.63405935\n",
      "Iteration 199, loss = 0.63764340\n",
      "Iteration 200, loss = 0.63387883\n",
      "Iteration 201, loss = 0.63803632\n",
      "Iteration 202, loss = 0.63719940\n",
      "Iteration 203, loss = 0.63231739\n",
      "Iteration 204, loss = 0.63383565\n",
      "Iteration 205, loss = 0.63377866\n",
      "Iteration 206, loss = 0.63421758\n",
      "Iteration 207, loss = 0.63425616\n",
      "Iteration 208, loss = 0.63157132\n",
      "Iteration 209, loss = 0.63364070\n",
      "Iteration 210, loss = 0.63119832\n",
      "Iteration 211, loss = 0.63279563\n",
      "Iteration 212, loss = 0.63477503\n",
      "Iteration 213, loss = 0.63278062\n",
      "Iteration 214, loss = 0.63125877\n",
      "Iteration 215, loss = 0.63093422\n",
      "Iteration 216, loss = 0.63135042\n",
      "Iteration 217, loss = 0.62790385\n",
      "Iteration 218, loss = 0.63104829\n",
      "Iteration 219, loss = 0.63193969\n",
      "Iteration 220, loss = 0.63040857\n",
      "Iteration 221, loss = 0.63015054\n",
      "Iteration 222, loss = 0.63041270\n",
      "Iteration 223, loss = 0.63024750\n",
      "Iteration 224, loss = 0.62922828\n",
      "Iteration 225, loss = 0.62762245\n",
      "Iteration 226, loss = 0.62946006\n",
      "Iteration 227, loss = 0.62772962\n",
      "Iteration 228, loss = 0.63063419\n",
      "Iteration 229, loss = 0.62589281\n",
      "Iteration 230, loss = 0.62719940\n",
      "Iteration 231, loss = 0.62773046\n",
      "Iteration 232, loss = 0.62688813\n",
      "Iteration 233, loss = 0.62843890\n",
      "Iteration 234, loss = 0.62738576\n",
      "Iteration 235, loss = 0.62490308\n",
      "Iteration 236, loss = 0.62654970\n",
      "Iteration 237, loss = 0.62626070\n",
      "Iteration 238, loss = 0.62659069\n",
      "Iteration 239, loss = 0.62474509\n",
      "Iteration 240, loss = 0.62794399\n",
      "Iteration 241, loss = 0.62591804\n",
      "Iteration 242, loss = 0.62466620\n",
      "Iteration 243, loss = 0.62628205\n",
      "Iteration 244, loss = 0.62529412\n",
      "Iteration 245, loss = 0.62284168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(50, 50, 50, 50, 50), max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo o R² de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8998590557155531"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o R² de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842424983497301"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_a = X_train.drop(['error','y'],axis=1).copy()\n",
    "x_a['X1'] = x_a['X1'] - 1\n",
    "c1 = x_a['X0']*x_a['X1']+x_a['X2']*x_a['X3']+x_a['X1']*x_a['X3'] + x4_train*((x_a['X1'])-x_a['X2'])\n",
    "cp = model.predict(x_a)\n",
    "mean_absolute_error(cp, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7744799114702224\n",
      "0.8111029669432286\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_train_pred,y_train))\n",
    "print(mean_absolute_error(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656860402480766"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos o erro de predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_error =   y_pred -y_test\n",
    "train_error = y_train - y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train['error'] = train_error\n",
    "X_test['error'] = model_error\n",
    "X_train['y'] = y_train\n",
    "X_test['y'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLPRegressor(hidden_layer_sizes=(50,50,50,50,50),activation='relu',max_iter=1000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23946352\n",
      "Iteration 2, loss = 0.70047077\n",
      "Iteration 3, loss = 0.69585336\n",
      "Iteration 4, loss = 0.68958915\n",
      "Iteration 5, loss = 0.68902416\n",
      "Iteration 6, loss = 0.68784487\n",
      "Iteration 7, loss = 0.68415625\n",
      "Iteration 8, loss = 0.68523093\n",
      "Iteration 9, loss = 0.68276539\n",
      "Iteration 10, loss = 0.68117536\n",
      "Iteration 11, loss = 0.67989042\n",
      "Iteration 12, loss = 0.68594808\n",
      "Iteration 13, loss = 0.67766621\n",
      "Iteration 14, loss = 0.68191878\n",
      "Iteration 15, loss = 0.68158384\n",
      "Iteration 16, loss = 0.67931302\n",
      "Iteration 17, loss = 0.67735899\n",
      "Iteration 18, loss = 0.67778460\n",
      "Iteration 19, loss = 0.67648863\n",
      "Iteration 20, loss = 0.67617562\n",
      "Iteration 21, loss = 0.67684742\n",
      "Iteration 22, loss = 0.67540041\n",
      "Iteration 23, loss = 0.67433559\n",
      "Iteration 24, loss = 0.67464206\n",
      "Iteration 25, loss = 0.67587394\n",
      "Iteration 26, loss = 0.67228320\n",
      "Iteration 27, loss = 0.67259681\n",
      "Iteration 28, loss = 0.67399284\n",
      "Iteration 29, loss = 0.67494452\n",
      "Iteration 30, loss = 0.67538855\n",
      "Iteration 31, loss = 0.67225622\n",
      "Iteration 32, loss = 0.67268998\n",
      "Iteration 33, loss = 0.67278215\n",
      "Iteration 34, loss = 0.67314100\n",
      "Iteration 35, loss = 0.67238708\n",
      "Iteration 36, loss = 0.67267106\n",
      "Iteration 37, loss = 0.67147212\n",
      "Iteration 38, loss = 0.67153776\n",
      "Iteration 39, loss = 0.66974941\n",
      "Iteration 40, loss = 0.67156936\n",
      "Iteration 41, loss = 0.66963904\n",
      "Iteration 42, loss = 0.66976963\n",
      "Iteration 43, loss = 0.66979534\n",
      "Iteration 44, loss = 0.67052837\n",
      "Iteration 45, loss = 0.66940293\n",
      "Iteration 46, loss = 0.66837101\n",
      "Iteration 47, loss = 0.67000031\n",
      "Iteration 48, loss = 0.66988446\n",
      "Iteration 49, loss = 0.66738568\n",
      "Iteration 50, loss = 0.66920569\n",
      "Iteration 51, loss = 0.66996517\n",
      "Iteration 52, loss = 0.66613666\n",
      "Iteration 53, loss = 0.66756163\n",
      "Iteration 54, loss = 0.66836130\n",
      "Iteration 55, loss = 0.66848873\n",
      "Iteration 56, loss = 0.66573188\n",
      "Iteration 57, loss = 0.66772205\n",
      "Iteration 58, loss = 0.67015548\n",
      "Iteration 59, loss = 0.66747372\n",
      "Iteration 60, loss = 0.66603518\n",
      "Iteration 61, loss = 0.66584659\n",
      "Iteration 62, loss = 0.66759471\n",
      "Iteration 63, loss = 0.66559621\n",
      "Iteration 64, loss = 0.66587175\n",
      "Iteration 65, loss = 0.66389943\n",
      "Iteration 66, loss = 0.66576517\n",
      "Iteration 67, loss = 0.66461196\n",
      "Iteration 68, loss = 0.66755602\n",
      "Iteration 69, loss = 0.66381928\n",
      "Iteration 70, loss = 0.66371493\n",
      "Iteration 71, loss = 0.66297698\n",
      "Iteration 72, loss = 0.66210172\n",
      "Iteration 73, loss = 0.66219452\n",
      "Iteration 74, loss = 0.66483042\n",
      "Iteration 75, loss = 0.66258147\n",
      "Iteration 76, loss = 0.66254939\n",
      "Iteration 77, loss = 0.66169421\n",
      "Iteration 78, loss = 0.66206515\n",
      "Iteration 79, loss = 0.66176900\n",
      "Iteration 80, loss = 0.66297036\n",
      "Iteration 81, loss = 0.66289205\n",
      "Iteration 82, loss = 0.66304820\n",
      "Iteration 83, loss = 0.66139108\n",
      "Iteration 84, loss = 0.66275746\n",
      "Iteration 85, loss = 0.66186784\n",
      "Iteration 86, loss = 0.66064241\n",
      "Iteration 87, loss = 0.66082755\n",
      "Iteration 88, loss = 0.66037743\n",
      "Iteration 89, loss = 0.66075569\n",
      "Iteration 90, loss = 0.65935804\n",
      "Iteration 91, loss = 0.66073233\n",
      "Iteration 92, loss = 0.65850803\n",
      "Iteration 93, loss = 0.66172898\n",
      "Iteration 94, loss = 0.66007036\n",
      "Iteration 95, loss = 0.66061370\n",
      "Iteration 96, loss = 0.65845068\n",
      "Iteration 97, loss = 0.65984047\n",
      "Iteration 98, loss = 0.65760574\n",
      "Iteration 99, loss = 0.65729419\n",
      "Iteration 100, loss = 0.65868748\n",
      "Iteration 101, loss = 0.65940134\n",
      "Iteration 102, loss = 0.65684490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(50, 50, 50, 50, 50), max_iter=1000, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo o R² de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9094159833128109"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o R² de teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9041873650650744"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X2_test,y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.48662154,  1.29517104,  2.12721936, ..., -1.64046134,\n",
       "        1.12664747, -4.88608108])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X2_train['X1'] = X2_train['X1'] + 1\n",
    "c2 = X2_train['X0']+X2_train['X1']+X2_train['X2']+X2_train['X3']+X2_train['X0']*X2_train['X1'] + x4_train*((X2_train['X1'])-X2_train['X2'])\n",
    "cp2 = model.predict(X2_train)\n",
    "mean_absolute_error(cp2, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred = model.predict(X2_test)\n",
    "y2_train_pred = model.predict(X2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0683131549642644\n",
      "2.0592811110696143\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y2_train_pred,y2_train))\n",
    "print(mean_absolute_error(y2_pred,y2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos o erro de predição."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x2_a = X2_train.drop(['error','y'],axis=1).copy()\n",
    "x2_a['X1'] = x2_a['X1'] - 1\n",
    "c2 = x2_a['X0']+x2_a['X1']+x2_a['X2']+x2_a['X3']+x2_a['X0']*x2_a['X1'] + x4_train*((x2_a['X1'])-x2_a['X2'])\n",
    "c2p = model.predict(x2_a)\n",
    "mean_absolute_error(c2p, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_error =   y2_pred -y2_test\n",
    "train2_error = y2_train - y2_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/guilherme/anaconda3/envs/tf36/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X2_train['error'] = train2_error\n",
    "X2_test['error'] = model2_error\n",
    "X2_train['y'] = y2_train\n",
    "X2_test['y'] = y2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoder_train = X_train.merge(X2_train,left_index=True,right_index=True)\n",
    "df_encoder_test = X_test.merge(X2_test,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_outs_train = df_encoder_train[['error_x','error_y','y_y','y_x']]\n",
    "df_outs_test = df_encoder_test[['error_x','error_y','y_y','y_x']]\n",
    "df_features_train = df_encoder_train.drop(['error_x','error_y','y_y','y_x'], axis = 1)\n",
    "df_features_test = df_encoder_test.drop(['error_x','error_y','y_y','y_x'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 1\n",
    "ncol = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = Input(shape = (ncol, ))\n",
    "input_dim2 = Input(shape = (ncol - 4, ))\n",
    "# Encoder Layers\n",
    "encoded1 = Dense(100, activation = 'relu')(input_dim)\n",
    "encoded2 = Dense(50, activation = 'relu')(encoded1)\n",
    "encoded3 = Dense(40, activation = 'relu')(encoded2)\n",
    "encoded4 = Dense(20, activation = 'relu')(encoded3)\n",
    "encoded5 = Dense(1, activation = 'relu')(encoded4)\n",
    "encoded13 = Dense(encoding_dim, activation = 'relu')(encoded5)\n",
    "merged = keras.layers.concatenate([encoded13, input_dim2], axis=-1)\n",
    "# Decoder Layers\n",
    "decoded1 = Dense(50, activation = 'tanh')(merged)\n",
    "decoded2 = Dense(100, activation = 'tanh')(decoded1)\n",
    "decoded3 = Dense(300, activation = 'tanh')(decoded2)\n",
    "decoded4 = Dense(100, activation = 'tanh')(decoded3)\n",
    "decoded5 = Dense(50, activation = 'tanh')(decoded4)\n",
    "decoded13 = Dense(4, activation = 'linear')(decoded5)\n",
    "\n",
    "# Combine Encoder and Deocder layers\n",
    "autoencoder = Model(inputs = [input_dim, input_dim2], outputs = [decoded13])\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the Model\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8574\n",
      "Epoch 2/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8522\n",
      "Epoch 3/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8492\n",
      "Epoch 4/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8465\n",
      "Epoch 5/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8453\n",
      "Epoch 6/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8416\n",
      "Epoch 7/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8413\n",
      "Epoch 8/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8392\n",
      "Epoch 9/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8365\n",
      "Epoch 10/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8364\n",
      "Epoch 11/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8340\n",
      "Epoch 12/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8342\n",
      "Epoch 13/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8326\n",
      "Epoch 14/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8317\n",
      "Epoch 15/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8305\n",
      "Epoch 16/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8296\n",
      "Epoch 17/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8286\n",
      "Epoch 18/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8281\n",
      "Epoch 19/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8280\n",
      "Epoch 20/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8267\n",
      "Epoch 21/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8274\n",
      "Epoch 22/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8253\n",
      "Epoch 23/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8259\n",
      "Epoch 24/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8247\n",
      "Epoch 25/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8243\n",
      "Epoch 26/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8223\n",
      "Epoch 27/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8225\n",
      "Epoch 28/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8209\n",
      "Epoch 29/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8205\n",
      "Epoch 30/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8204\n",
      "Epoch 31/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8206\n",
      "Epoch 32/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8200\n",
      "Epoch 33/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8194\n",
      "Epoch 34/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8197\n",
      "Epoch 35/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8193\n",
      "Epoch 36/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8178\n",
      "Epoch 37/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8174\n",
      "Epoch 38/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8178\n",
      "Epoch 39/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8162\n",
      "Epoch 40/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8165\n",
      "Epoch 41/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8168\n",
      "Epoch 42/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8157\n",
      "Epoch 43/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8140\n",
      "Epoch 44/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8142\n",
      "Epoch 45/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8133\n",
      "Epoch 46/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8132\n",
      "Epoch 47/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8129\n",
      "Epoch 48/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8122\n",
      "Epoch 49/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8113\n",
      "Epoch 50/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8121\n",
      "Epoch 51/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8112\n",
      "Epoch 52/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8098\n",
      "Epoch 53/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8112\n",
      "Epoch 54/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8095\n",
      "Epoch 55/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8096\n",
      "Epoch 56/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8088\n",
      "Epoch 57/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8073\n",
      "Epoch 58/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8068\n",
      "Epoch 59/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8071\n",
      "Epoch 60/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8074\n",
      "Epoch 61/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8066\n",
      "Epoch 62/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8048\n",
      "Epoch 63/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8046\n",
      "Epoch 64/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8042\n",
      "Epoch 65/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8037\n",
      "Epoch 66/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8042\n",
      "Epoch 67/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8038\n",
      "Epoch 68/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8030\n",
      "Epoch 69/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.8037\n",
      "Epoch 70/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8018\n",
      "Epoch 71/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8015\n",
      "Epoch 72/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8013\n",
      "Epoch 73/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8008\n",
      "Epoch 74/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.8010\n",
      "Epoch 75/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7992\n",
      "Epoch 76/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7997\n",
      "Epoch 77/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7984\n",
      "Epoch 78/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7988\n",
      "Epoch 79/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7981\n",
      "Epoch 80/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7974\n",
      "Epoch 81/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7969\n",
      "Epoch 82/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7974\n",
      "Epoch 83/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7960\n",
      "Epoch 84/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7953\n",
      "Epoch 85/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7950\n",
      "Epoch 86/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7948\n",
      "Epoch 87/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7946\n",
      "Epoch 88/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7941\n",
      "Epoch 89/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7941\n",
      "Epoch 90/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7929\n",
      "Epoch 91/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7926\n",
      "Epoch 92/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7920\n",
      "Epoch 93/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7929\n",
      "Epoch 94/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7931\n",
      "Epoch 95/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7911\n",
      "Epoch 96/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7912\n",
      "Epoch 97/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7897\n",
      "Epoch 98/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7882\n",
      "Epoch 99/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7885\n",
      "Epoch 100/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7888\n",
      "Epoch 101/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7896\n",
      "Epoch 102/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7878\n",
      "Epoch 103/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7876\n",
      "Epoch 104/1500\n",
      "2094/2094 [==============================] - 4s 2ms/step - loss: 0.7861\n",
      "Epoch 105/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7862\n",
      "Epoch 106/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7865\n",
      "Epoch 107/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7870\n",
      "Epoch 108/1500\n",
      "2094/2094 [==============================] - 3s 2ms/step - loss: 0.7855\n",
      "Epoch 109/1500\n",
      "2039/2094 [============================>.] - ETA: 0s - loss: 0.7846"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0,\n",
    "                              patience=50,\n",
    "                              verbose=0, mode='auto',\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = [es]\n",
    "autoencoder.fit(x = [df_encoder_train, df_features_train], y = df_outs_train, epochs = 1500, \n",
    "                batch_size = 32, shuffle = True,\n",
    "                \n",
    "               callbacks = callbacks\n",
    "                \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs = [input_dim,input_dim2], outputs = encoded13)\n",
    "encoded_input = Input(shape = (encoding_dim, ))\n",
    "encoded_train = pd.DataFrame(encoder.predict([df_encoder_train,df_features_train]))\n",
    "encoded_train = encoded_train.add_prefix('feature_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(encoded_train['feature_2'], df_outs_train['y_y']))\n",
    "print(mean_absolute_error(encoded_train['feature_3'], df_outs_train['y_x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(encoded_train['feature_2'], df_outs_train['y_y']))\n",
    "print(r2_score(encoded_train['feature_3'], df_outs_train['y_x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_and_check(df_features_train_ori,feat,modifier):\n",
    "    df_features_train = df_features_train_ori.copy()\n",
    "    df_features_train[feat] = df_features_train[feat] + modifier\n",
    "    encoded_train = pd.DataFrame(encoder.predict([df_encoder_train,df_features_train]))\n",
    "    encoded_train = encoded_train.add_prefix('feature_')\n",
    "    X_t = df_features_train\n",
    "    contra = X_t['X0_x']*X_t['X1_x']+X_t['X2_x']*X_t['X3_x']+X_t['X1_x']*X_t['X3_x'] + x4_train*((X_t['X1_x'])-X_t['X2_x'])\n",
    "    contra2 = X_t['X0_y']+X_t['X1_y']+X_t['X2_y']+X_t['X3_y']+X_t['X0_y']*X_t['X1_y'] + x4_train*((X_t['X1_y'])-X_t['X2_y'])\n",
    "    print('MAE C1: ' + str(mean_absolute_error(encoded_train['feature_3'], contra)))\n",
    "    print('R2 C1: ' + str(r2_score(contra, encoded_train['feature_3'])))\n",
    "    print('MAE C2: ' + str(mean_absolute_error( encoded_train['feature_2'],contra2)))\n",
    "    print('R2 C2: ' + str(r2_score(contra2, encoded_train['feature_2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_and_check(df_features_train,'X1_y',-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_and_check(df_features_train,'X1_y',+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_and_check(df_features_train,'X1_x',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_and_check(df_features_train,'X1_x',+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ys_train.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_absolute_error(contra, encoded_train['feature_3'])\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_sum = 0\n",
    "for real, prediction in zip(contra, encoded_train['feature_3']):\n",
    "    mpe_sum += ((real - prediction)/real)\n",
    "mpe = mpe_sum/len(contra)\n",
    "print(mpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse / np.mean(contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse / np.std(contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoder_train = X_train.merge(X2_train,left_index=True,right_index=True)\n",
    "df_encoder_test = X_test.merge(X2_test,left_index=True,right_index=True)\n",
    "df_outs_train = df_encoder_train[['error_x','error_y','y_y','y_x']]\n",
    "df_outs_test = df_encoder_test[['error_x','error_y','y_y','y_x']]\n",
    "df_features_train = df_encoder_train.drop(['error_x','error_y','y_y','y_x'], axis = 1)\n",
    "df_features_test = df_encoder_test.drop(['error_x','error_y','y_y','y_x'], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf36]",
   "language": "python",
   "name": "conda-env-tf36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
